{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Random Forest for Human Activity Recognition using the Capture-24 dataset\n",
    "## Introduction\n",
    "Here is the implementation of a Random Forest model for Human Activity Recognition (HAR) using the Capture-24 dataset.\n",
    "## Imports"
   ],
   "id": "106210eee199af2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy.signal as signal\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, log_loss, ConfusionMatrixDisplay\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from tqdm import tqdm"
   ],
   "id": "d08b96788892cc61"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Preparation\n",
    "Here define some functions, that will help us later.\n",
    "\n",
    "### map_labels\n",
    "This function will map our labels to a set of labels. The possible set of labels can be chosen from the annotation-label-dictionary. This is useful, as the original labels can be too specific.\n",
    "\n",
    "### extract_windows\n",
    "This function will extract a window from the original continuous data. This is necessary, as we want fixed-size windows for our classification task. Here different sizes can be chosen, we will use 10s windows.\n"
   ],
   "id": "dd669585596d7c5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "anno_label_dict = pd.read_csv('data/annotation-label-dictionary.csv',\n",
    "                              index_col='annotation', dtype='string')\n",
    "print(\"Annotation-Label Dictionary\")\n",
    "print(anno_label_dict)\n",
    "\n",
    "\"Source: https://github.com/OxWearables/capture24\"\n",
    "\n",
    "\n",
    "# Map the original labels to a set of labels.\n",
    "def map_labels(data, anno_label_dict, label_mapping):\n",
    "    data['label'] = (anno_label_dict[label_mapping]\n",
    "                     .reindex(data['annotation'])\n",
    "                     .to_numpy())\n",
    "    return data\n",
    "\n",
    "\n",
    "# Extract a specific-sized window.\n",
    "def extract_windows(data, winsize=10):\n",
    "    X, Y = [], []\n",
    "    for t, w in data.resample(f'{winsize}s', origin='start'):\n",
    "\n",
    "        # Check window has no NaNs and is of correct length\n",
    "        # 10s @ 100Hz = 1000 ticks\n",
    "        if w.isna().any().any() or len(w) != winsize * 100:\n",
    "            continue\n",
    "\n",
    "        x = w[['x', 'y', 'z']].to_numpy()\n",
    "        y = w['label'].mode(dropna=False).item()\n",
    "\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n",
    "    X = np.stack(X)\n",
    "    Y = np.stack(Y)\n",
    "\n",
    "    return X, Y\n"
   ],
   "id": "1637ff9a0bf2b4cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Feature Extraction\n",
    "\n",
    "Here we define a function that will extract features from a window of data. The features are commonly used in Human Activity Recognition (HAR) tasks. The features are:\n",
    "- Minimum, 25th percentile, median, 75th percentile, maximum of x, y, z\n",
    "- Correlation between x, y, z\n",
    "- 1s autocorrelation\n",
    "- Angular features: roll, pitch, yaw\n",
    "- Spectral entropy, 1st and 2nd dominant frequencies\n",
    "- Peak features: number of peaks, peak prominence\n",
    "These features are the usual features used in HAR tasks."
   ],
   "id": "3c501d6b1e7072e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"Source: https://github.com/OxWearables/capture24\"\n",
    "\n",
    "\n",
    "def extract_features(xyz, sample_rate=100):\n",
    "    \"\"\"Extract commonly used HAR time-series features. xyz is a window of shape (N,3)\"\"\"\n",
    "\n",
    "    feats = {}\n",
    "\n",
    "    x, y, z = xyz.T\n",
    "\n",
    "    feats['xmin'], feats['xq25'], feats['xmed'], feats['xq75'], feats['xmax'] = np.quantile(x, (0, .25, .5, .75, 1))\n",
    "    feats['ymin'], feats['yq25'], feats['ymed'], feats['yq75'], feats['ymax'] = np.quantile(y, (0, .25, .5, .75, 1))\n",
    "    feats['zmin'], feats['zq25'], feats['zmed'], feats['zq75'], feats['zmax'] = np.quantile(z, (0, .25, .5, .75, 1))\n",
    "\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):  # ignore div by 0 warnings\n",
    "        # xy, xy, zx correlation\n",
    "        feats['xycorr'] = np.nan_to_num(np.corrcoef(x, y)[0, 1])\n",
    "        feats['yzcorr'] = np.nan_to_num(np.corrcoef(y, z)[0, 1])\n",
    "        feats['zxcorr'] = np.nan_to_num(np.corrcoef(z, x)[0, 1])\n",
    "\n",
    "    v = np.linalg.norm(xyz, axis=1)\n",
    "\n",
    "    feats['min'], feats['q25'], feats['med'], feats['q75'], feats['max'] = np.quantile(v, (0, .25, .5, .75, 1))\n",
    "\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):  # ignore div by 0 warnings\n",
    "        # 1s autocorrelation\n",
    "        feats['corr1s'] = np.nan_to_num(np.corrcoef(v[:-sample_rate], v[sample_rate:]))[0, 1]\n",
    "\n",
    "    # Angular features\n",
    "    feats.update(angular_features(xyz, sample_rate))\n",
    "\n",
    "    # Spectral features\n",
    "    feats.update(spectral_features(v, sample_rate))\n",
    "\n",
    "    # Peak features\n",
    "    feats.update(peak_features(v, sample_rate))\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def spectral_features(v, sample_rate):\n",
    "    \"\"\" Spectral entropy, 1st & 2nd dominant frequencies \"\"\"\n",
    "\n",
    "    feats = {}\n",
    "\n",
    "    # Spectrum using Welch's method with 3s segment length\n",
    "    # First run without detrending to get the true spectrum\n",
    "    freqs, powers = signal.welch(v, fs=sample_rate,\n",
    "                                 nperseg=3 * sample_rate,\n",
    "                                 noverlap=2 * sample_rate,\n",
    "                                 detrend=False,\n",
    "                                 average='median')\n",
    "\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):  # ignore div by 0 warnings\n",
    "        feats['pentropy'] = np.nan_to_num(stats.entropy(powers + 1e-16))\n",
    "\n",
    "    # Spectrum using Welch's method with 3s segment length\n",
    "    # Now do detrend to focus on the relevant freqs\n",
    "    freqs, powers = signal.welch(v, fs=sample_rate,\n",
    "                                 nperseg=3 * sample_rate,\n",
    "                                 noverlap=2 * sample_rate,\n",
    "                                 detrend='constant',\n",
    "                                 average='median')\n",
    "\n",
    "    peaks, _ = signal.find_peaks(powers)\n",
    "    peak_powers = powers[peaks]\n",
    "    peak_freqs = freqs[peaks]\n",
    "    peak_ranks = np.argsort(peak_powers)[::-1]\n",
    "    if len(peaks) >= 2:\n",
    "        feats['f1'] = peak_freqs[peak_ranks[0]]\n",
    "        feats['f2'] = peak_freqs[peak_ranks[1]]\n",
    "        feats['p1'] = peak_powers[peak_ranks[0]]\n",
    "        feats['p2'] = peak_powers[peak_ranks[1]]\n",
    "    elif len(peaks) == 1:\n",
    "        feats['f1'] = feats['f2'] = peak_freqs[peak_ranks[0]]\n",
    "        feats['p1'] = feats['p2'] = peak_powers[peak_ranks[0]]\n",
    "    else:\n",
    "        feats['f1'] = feats['f2'] = 0\n",
    "        feats['p1'] = feats['p2'] = 0\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def peak_features(v, sample_rate):\n",
    "    \"\"\" Features of the signal peaks. A proxy to step counts. \"\"\"\n",
    "\n",
    "    feats = {}\n",
    "    u = butterfilt(v, (.6, 5), fs=sample_rate)\n",
    "    peaks, peak_props = signal.find_peaks(u, distance=0.2 * sample_rate, prominence=0.25)\n",
    "    feats['numPeaks'] = len(peaks)\n",
    "    if len(peak_props['prominences']) > 0:\n",
    "        feats['peakPromin'] = np.median(peak_props['prominences'])\n",
    "    else:\n",
    "        feats['peakPromin'] = 0\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def angular_features(xyz, sample_rate):\n",
    "    \"\"\" Roll, pitch, yaw.\n",
    "    Hip and Wrist Accelerometer Algorithms for Free-Living Behavior\n",
    "    Classification, Ellis et al.\n",
    "    \"\"\"\n",
    "\n",
    "    feats = {}\n",
    "\n",
    "    # Raw angles\n",
    "    x, y, z = xyz.T\n",
    "\n",
    "    roll = np.arctan2(y, z)\n",
    "    pitch = np.arctan2(x, z)\n",
    "    yaw = np.arctan2(y, x)\n",
    "\n",
    "    feats['avgroll'] = np.mean(roll)\n",
    "    feats['avgpitch'] = np.mean(pitch)\n",
    "    feats['avgyaw'] = np.mean(yaw)\n",
    "    feats['sdroll'] = np.std(roll)\n",
    "    feats['sdpitch'] = np.std(pitch)\n",
    "    feats['sdyaw'] = np.std(yaw)\n",
    "\n",
    "    # Gravity angles\n",
    "    xyz = butterfilt(xyz, 0.5, fs=sample_rate)\n",
    "\n",
    "    x, y, z = xyz.T\n",
    "\n",
    "    roll = np.arctan2(y, z)\n",
    "    pitch = np.arctan2(x, z)\n",
    "    yaw = np.arctan2(y, x)\n",
    "\n",
    "    feats['rollg'] = np.mean(roll)\n",
    "    feats['pitchg'] = np.mean(pitch)\n",
    "    feats['yawg'] = np.mean(yaw)\n",
    "\n",
    "    return feats\n",
    "\n",
    "# Butterworth filter\n",
    "def butterfilt(x, cutoffs, fs, order=10, axis=0):\n",
    "    nyq = 0.5 * fs\n",
    "    if isinstance(cutoffs, tuple):\n",
    "        hicut, lowcut = cutoffs\n",
    "        if hicut > 0:\n",
    "            btype = 'bandpass'\n",
    "            Wn = (hicut / nyq, lowcut / nyq)\n",
    "        else:\n",
    "            btype = 'low'\n",
    "            Wn = lowcut / nyq\n",
    "    else:\n",
    "        btype = 'low'\n",
    "        Wn = cutoffs / nyq\n",
    "    sos = signal.butter(order, Wn, btype=btype, analog=False, output='sos')\n",
    "    y = signal.sosfiltfilt(sos, x, axis=axis)\n",
    "    return y\n",
    "\n",
    "\n",
    "def get_feature_names():\n",
    "    \"\"\" Hacky way to get the list of feature names \"\"\"\n",
    "\n",
    "    feats = extract_features(np.zeros((1000, 3)), 100)\n",
    "    return list(feats.keys())\n"
   ],
   "id": "54311bc61951ad7d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train the model\n",
    "\n",
    "### Load the data"
   ],
   "id": "9e97b9a514cf9b0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check the contents of the data directory\n",
    "print('Content of data/')\n",
    "print(sorted(os.listdir('data/')))\n",
    "\n",
    "# Define the label mapping\n",
    "label_mapping = \"label:Willetts2018\"\n",
    "\n",
    "# Load the metadata file\n",
    "metadata = pd.read_csv('data/metadata.csv')\n",
    "\n",
    "# Load and concatenate data from all files\n",
    "data_list = []\n",
    "for file in sorted(os.listdir('data/')):\n",
    "    if not file.endswith('csv.gz'):\n",
    "        continue\n",
    "\n",
    "    data = pd.read_csv(f'data/{file}', index_col='time', parse_dates=['time'],\n",
    "                       dtype={'x': 'f4', 'y': 'f4', 'z': 'f4', 'annotation': 'string'})\n",
    "\n",
    "    file_id = file.split('.')[0]\n",
    "    print('Current file:', file_id)\n",
    "\n",
    "    # Find the metadata for this file\n",
    "    file_metadata = metadata[metadata['pid'] == file_id]\n",
    "\n",
    "    # Add metadata to the data\n",
    "    if not file_metadata.empty:\n",
    "        for col in ['age', 'sex']:\n",
    "            if col in file_metadata.columns:\n",
    "                data[col] = file_metadata.iloc[0][col]\n",
    "\n",
    "    # Map the label\n",
    "    map_labels(data, anno_label_dict, label_mapping)\n",
    "    data_list.append(data)\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for data in data_list:\n",
    "    X_, Y_ = extract_windows(data)\n",
    "    X.append(X_)\n",
    "    Y.append(Y_)\n",
    "\n",
    "X_feats = []\n",
    "\n",
    "for X_ in X:\n",
    "    X_feats.append(pd.DataFrame([extract_features(x) for x in X_]))\n",
    "\n",
    "X_feats = pd.concat(X_feats, ignore_index=True)\n",
    "Y = np.concatenate(Y, axis=0)\n",
    "\n",
    "print('Data preprocessing done.\\n')"
   ],
   "id": "c6c7fb0b02916a89"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Split the data\n",
    "Here we split the data into training and evaluation sets. We will use a 90/10 split."
   ],
   "id": "12352d391b77d18f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Split the data into training and evaluation sets (90% train, 10% eval)\n",
    "X_train, X_eval, Y_train, Y_eval = train_test_split(X_feats, Y, test_size=0.1, random_state=0)\n",
    "\n",
    "# Preprocess the features\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_eval_scaled = scaler.transform(X_eval)"
   ],
   "id": "c23a47d1134e0439"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Defining the model\n",
    "Here we define the model. The hyperparameters are chosen based on a previous grid search. As the dataset is very imbalanced (especially regarding sleep labels) to prevent overfitting, we use a BalancedRandomForestClassifier."
   ],
   "id": "ddddf781618c6edd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the model\n",
    "clf = BalancedRandomForestClassifier(random_state=0, max_depth=None, max_features='sqrt', n_estimators=300,\n",
    "                                     min_samples_leaf=1, min_samples_split=2, sampling_strategy='all', replacement=True, bootstrap=False)"
   ],
   "id": "f55086ecb768d27e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Cross-validation\n",
    "Here we perform K-fold cross-validation to evaluate the model. We will use 5 folds.\n",
    "As the dataset is relatively big and the training can take a long time, we tqdm to show a progress bar."
   ],
   "id": "80f690da4f685d9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cross-validation with progress bar\n",
    "cv_scores = []\n",
    "for _ in tqdm(range(5), desc=\"Cross-validation progress\"):  # Progress bar for CV\n",
    "    score = cross_val_score(clf, X_train_scaled, Y_train, cv=5, scoring='accuracy')\n",
    "    cv_scores.append(score)\n",
    "\n",
    "cv_scores = np.array(cv_scores)\n",
    "\n",
    "# Print the results\n",
    "print(f'Cross-validation scores: {cv_scores}')\n",
    "print(f'Mean cross-validation score: {cv_scores.mean()}')\n",
    "print(f'Standard deviation of cross-validation score: {cv_scores.std()}')"
   ],
   "id": "df5a3a53cda4a12e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Evaluate the model\n",
    "Here we evaluate the model on the evaluation dataset.\n",
    "We calculate the following metrics:\n",
    "- Accuracy\n",
    "- F1 score\n",
    "- Precision\n",
    "- Recall\n",
    "- Log loss\n",
    "- Confusion matrix"
   ],
   "id": "90fcdbf3a26e244b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Fit the model on the training dataset\n",
    "clf.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# Evaluate on the last dataset\n",
    "X_eval_scaled = scaler.transform(X_eval)\n",
    "Y_pred = clf.predict(X_eval_scaled)\n",
    "\n",
    "# Calculate the important metrics\n",
    "accuracy = accuracy_score(Y_eval, Y_pred)\n",
    "f1_score = f1_score(Y_eval, Y_pred, average='weighted')\n",
    "precision = precision_score(Y_eval, Y_pred, average='weighted')\n",
    "recall = recall_score(Y_eval, Y_pred, average='weighted')\n",
    "log_loss = log_loss(Y_eval, Y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 score: {clf.score(X_eval_scaled, Y_eval)}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'Log loss: {log_loss}')\n",
    "print(f'Confusion matrix: {clf.classes_}')\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_predictions(\n",
    "    Y_eval,\n",
    "    Y_pred,\n",
    "    display_labels=clf.classes_\n",
    ")\n",
    "disp.ax_.set_title(\"Confusion matrix\")\n",
    "\n",
    "print(\"Confusion matrix\")\n",
    "print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ],
   "id": "32db4973cd83ed17"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
